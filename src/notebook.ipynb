{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 -qq install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    \n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(scores, y):\n",
    "    \"\"\"\n",
    "    Compute the multi-class SVM loss.\n",
    "    scores: numpy array of shape (N, C)\n",
    "    y: numpy array of shape (N,) with labels in range 0...C-1\n",
    "    Returns the average loss and the margin matrix.\n",
    "    \"\"\"\n",
    "    N = scores.shape[0]\n",
    "    \n",
    "    # Selecting the correct class scores\n",
    "    correct_class_scores = scores[np.arange(N), y].reshape(-1, 1)\n",
    "    \n",
    "    # Computing margins for all classes\n",
    "    margins = np.maximum(0, scores - correct_class_scores + 1)\n",
    "    margins[np.arange(N), y] = 0  # Excluding the correct class in loss\n",
    "    loss = np.sum(margins) / N\n",
    "    \n",
    "    return loss, margins\n",
    "\n",
    "def svm_loss_gradient(margins, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the SVM loss with respect to scores.\n",
    "    \"\"\"\n",
    "    N = margins.shape[0]\n",
    "    binary = (margins > 0).astype(float)\n",
    "    row_sum = np.sum(binary, axis=1)\n",
    "    binary[np.arange(N), y] = -row_sum\n",
    "    dScores = binary / N\n",
    "    \n",
    "    return dScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_dim, hidden_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a network with one hidden layer.\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(hidden_dim, input_dim) * 0.01\n",
    "    b1 = np.zeros((hidden_dim, 1))\n",
    "    W2 = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "    b2 = np.zeros((output_dim, 1))\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2, activation_func):\n",
    "    \"\"\"\n",
    "    Compute the forward pass through the network.\n",
    "    Returns: Z1 (pre-activation), A1 (activation output), and scores.\n",
    "    \"\"\"\n",
    "    Z1 = np.dot(X, W1.T) + b1.T\n",
    "    A1 = activation_func(Z1)\n",
    "    scores = np.dot(A1, W2.T) + b2.T  # raw output scores for each class\n",
    "\n",
    "    return Z1, A1, scores\n",
    "\n",
    "def backward(X, y, Z1, A1, scores, W2, activation_deriv, margins):\n",
    "    \"\"\"\n",
    "    Compute the backward pass through the network.\n",
    "    Returns gradients for W1, b1, W2, and b2.\n",
    "    \"\"\"\n",
    "    # Gradient of loss with respect to scores\n",
    "    dScores = svm_loss_gradient(margins, y)\n",
    "\n",
    "    # print(dScores.shape)    # For, testing\n",
    "    \n",
    "    # Gradients for the output layer parameters\n",
    "    dW2 = np.dot(A1.T, dScores).T\n",
    "    db2 = np.sum(dScores, axis=0, keepdims=True).T\n",
    "    dX2 = np.dot(dScores, W2)\n",
    "\n",
    "    # print(db2.shape)    # For, testing\n",
    "    # print(dW2.shape)   \n",
    "    \n",
    "    # Backprop into activation function\n",
    "    # dA1 = activation_deriv(dX2)\n",
    "    \n",
    "    dA1 = np.dot(dScores, W2)      # This is the gradient flowing back from the output.\n",
    "    dZ1 = dA1 * activation_deriv(Z1)  # Multiply by the derivative evaluated at Z1.\n",
    "\n",
    "\n",
    "    # Backprop into hidden layer\n",
    "    # dZ1 = np.dot(dA1, activation_deriv(Z1).T).T\n",
    "    # dW1 = np.dot(dA1.T, X)\n",
    "    # db1 = np.sum(dA1, axis=0, keepdims=True).T\n",
    "\n",
    "    dW1 = np.dot(dZ1.T, X)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True).T\n",
    "\n",
    "    # print(dW1.shape)    # For, testing\n",
    "    # print(db1.shape)\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using a simple gradient descent step.\n",
    "    \"\"\"\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2, activation_func):\n",
    "    \"\"\"\n",
    "    Predict class labels for input data X.\n",
    "    \"\"\"\n",
    "    _, _, scores = forward(X, W1, b1, W2, b2, activation_func)\n",
    "\n",
    "    return np.argmax(scores, axis=1)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute the accuracy given predictions and true labels.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X, y, activation='relu', epochs=1000, learning_rate=1e-2,\n",
    "                  hidden_dim=32, print_every=10):\n",
    "    \"\"\"\n",
    "    Train the neural network.\n",
    "    Parameters:\n",
    "      X: Input data of shape (N, input_dim)\n",
    "      y: Labels (N,)\n",
    "      activation: Choice of activation function ('relu', 'sigmoid', 'tanh')\n",
    "      epochs: Number of training iterations\n",
    "      learning_rate: Gradient descent learning rate\n",
    "      hidden_dim: Number of neurons in the hidden layer\n",
    "      print_every: Print loss and accuracy every 'print_every' epochs\n",
    "    Returns trained parameters: W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    activation_funcs = {\n",
    "        'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "        'tanh': (tanh, tanh_derivative),\n",
    "        'relu': (relu, relu_derivative)\n",
    "    }\n",
    "\n",
    "    if activation not in activation_funcs:\n",
    "        raise ValueError(\"Unsupported activation function. Choose from 'relu', 'sigmoid', or 'tanh'.\")\n",
    "    \n",
    "    act_func, act_deriv = activation_funcs[activation]\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = 10  # CIFAR-10 has 10 classes\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        # Forward pass\n",
    "        Z1, A1, scores = forward(X, W1, b1, W2, b2, act_func)\n",
    "        loss, margins = svm_loss(scores, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        dW1, db1, dW2, db2 = backward(X, y, Z1, A1, scores, W2, act_deriv, margins)\n",
    "        \n",
    "        # Updating parameters\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            y_pred = predict(X, W1, b1, W2, b2, act_func)\n",
    "            acc = compute_accuracy(y_pred, y)\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {acc:.4f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_data(path):\n",
    "    \"\"\"\n",
    "    Loads the CIFAR-10 dataset from the given directory.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the CIFAR-10 dataset directory.\n",
    "    \n",
    "    Returns:\n",
    "        X_train (numpy array): Training images of shape (N, 3072) and normalized.\n",
    "        y_train (numpy array): Training labels of shape (N,).\n",
    "        X_test (numpy array): Testing images of shape (N, 3072) and normalized.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading training labels\n",
    "    labels_path = os.path.join(path, \"train_labels.csv\")\n",
    "    labels_df = pd.read_csv(labels_path)\n",
    "    \n",
    "    # Extracting filenames and labels\n",
    "    train_filenames = labels_df['id'].values\n",
    "    train_labels = labels_df['label'].values\n",
    "    \n",
    "    train_dir = os.path.join(path, \"train\")\n",
    "    test_dir = os.path.join(path, \"test\")\n",
    "    \n",
    "    # Loading training images\n",
    "    X_train = []\n",
    "    \n",
    "    for filename in train_filenames:\n",
    "        img_path = os.path.join(train_dir, filename)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img, dtype=np.float32) / 255.0  # Normalizing pixels to [0,1]\n",
    "        X_train.append(img.flatten())  # Flattening to (3072,)\n",
    "    \n",
    "    X_train = np.array(X_train)  # Shape: (N, 3072)\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Encoding labels (convert category names to numerical values)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(train_labels)  # Converts to integers (0-9)\n",
    "    \n",
    "    # Loading test images\n",
    "    X_test = []\n",
    "    test_filenames = sorted(os.listdir(test_dir))  # Assuming all images in test directory\n",
    "    \n",
    "    for filename in test_filenames:\n",
    "        img_path = os.path.join(test_dir, filename)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        X_test.append(img.flatten())\n",
    "    \n",
    "    X_test = np.array(X_test)  # Shape: (M, 3072)\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ../data/cifar-10.zip -d ../data/cifar-10\n",
    "\n",
    "import py7zr\n",
    "\n",
    "with py7zr.SevenZipFile('../data/cifar-10/train.7z', mode='r') as archive:\n",
    "    archive.extractall(path=\"../data/cifar-10/train\")  \n",
    "\n",
    "with py7zr.SevenZipFile('../data/cifar-10/test.7z', mode='r') as archive:\n",
    "    archive.extractall(path=\"../data/cifar-10/test\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with ReLU activation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_544\\996942389.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2d5e55464c4d9d95860124a5378d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 9.0046, Accuracy = 0.1500\n",
      "Epoch 10: Loss = 7.0999, Accuracy = 0.2400\n",
      "Epoch 20: Loss = 6.9992, Accuracy = 0.1600\n",
      "Epoch 30: Loss = 6.9588, Accuracy = 0.1700\n",
      "Epoch 40: Loss = 6.2982, Accuracy = 0.3100\n",
      "Epoch 50: Loss = 5.3769, Accuracy = 0.6100\n",
      "Epoch 60: Loss = 3.6907, Accuracy = 0.8000\n",
      "Epoch 70: Loss = 4.0706, Accuracy = 0.5300\n",
      "Epoch 80: Loss = 3.3932, Accuracy = 0.3100\n",
      "Epoch 90: Loss = 1.0197, Accuracy = 0.8600\n",
      "Epoch 100: Loss = 1.2522, Accuracy = 0.2900\n",
      "Epoch 110: Loss = 0.0119, Accuracy = 1.0000\n",
      "Epoch 120: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 130: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 140: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 150: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 160: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 170: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 180: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 190: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 200: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 210: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 220: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 230: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 240: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 250: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 260: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 270: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 280: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 290: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 300: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 310: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 320: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 330: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 340: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 350: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 360: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 370: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 380: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 390: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 400: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 410: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 420: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 430: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 440: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 450: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 460: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 470: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 480: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 490: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 500: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 510: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 520: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 530: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 540: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 550: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 560: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 570: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 580: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 590: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 600: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 610: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 620: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 630: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 640: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 650: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 660: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 670: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 680: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 690: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 700: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 710: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 720: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 730: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 740: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 750: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 760: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 770: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 780: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 790: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 800: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 810: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 820: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 830: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 840: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 850: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 860: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 870: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 880: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 890: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 900: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 910: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 920: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 930: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 940: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 950: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 960: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 970: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 980: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 990: Loss = 0.0000, Accuracy = 1.0000\n",
      "\n",
      "Training with Sigmoid activation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f83c6b2096c4ba2a9669ca2b3083c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 8.9371, Accuracy = 0.1400\n",
      "Epoch 10: Loss = 7.2178, Accuracy = 0.1500\n",
      "Epoch 20: Loss = 7.1360, Accuracy = 0.1600\n",
      "Epoch 30: Loss = 7.0873, Accuracy = 0.1500\n",
      "Epoch 40: Loss = 7.0979, Accuracy = 0.1600\n",
      "Epoch 50: Loss = 7.0451, Accuracy = 0.1700\n",
      "Epoch 60: Loss = 7.0655, Accuracy = 0.1600\n",
      "Epoch 70: Loss = 7.0235, Accuracy = 0.1700\n",
      "Epoch 80: Loss = 6.9957, Accuracy = 0.1600\n",
      "Epoch 90: Loss = 6.9414, Accuracy = 0.1700\n",
      "Epoch 100: Loss = 6.9072, Accuracy = 0.3200\n",
      "Epoch 110: Loss = 6.8656, Accuracy = 0.3700\n",
      "Epoch 120: Loss = 6.8273, Accuracy = 0.3500\n",
      "Epoch 130: Loss = 6.7560, Accuracy = 0.4300\n",
      "Epoch 140: Loss = 6.6907, Accuracy = 0.4100\n",
      "Epoch 150: Loss = 6.6467, Accuracy = 0.4700\n",
      "Epoch 160: Loss = 6.5435, Accuracy = 0.5500\n",
      "Epoch 170: Loss = 6.4455, Accuracy = 0.5600\n",
      "Epoch 180: Loss = 6.3593, Accuracy = 0.5700\n",
      "Epoch 190: Loss = 6.2164, Accuracy = 0.5700\n",
      "Epoch 200: Loss = 6.0906, Accuracy = 0.5700\n",
      "Epoch 210: Loss = 5.9234, Accuracy = 0.5700\n",
      "Epoch 220: Loss = 5.7396, Accuracy = 0.5900\n",
      "Epoch 230: Loss = 5.5073, Accuracy = 0.6600\n",
      "Epoch 240: Loss = 5.2701, Accuracy = 0.6800\n",
      "Epoch 250: Loss = 4.9773, Accuracy = 0.6900\n",
      "Epoch 260: Loss = 4.6278, Accuracy = 0.6900\n",
      "Epoch 270: Loss = 4.2166, Accuracy = 0.6900\n",
      "Epoch 280: Loss = 3.7563, Accuracy = 0.7500\n",
      "Epoch 290: Loss = 3.2204, Accuracy = 0.8500\n",
      "Epoch 300: Loss = 2.7325, Accuracy = 0.8900\n",
      "Epoch 310: Loss = 2.2522, Accuracy = 0.9100\n",
      "Epoch 320: Loss = 1.7755, Accuracy = 0.9100\n",
      "Epoch 330: Loss = 1.5080, Accuracy = 0.9300\n",
      "Epoch 340: Loss = 1.2040, Accuracy = 0.9300\n",
      "Epoch 350: Loss = 0.9052, Accuracy = 1.0000\n",
      "Epoch 360: Loss = 0.7056, Accuracy = 1.0000\n",
      "Epoch 370: Loss = 0.4429, Accuracy = 1.0000\n",
      "Epoch 380: Loss = 0.3733, Accuracy = 1.0000\n",
      "Epoch 390: Loss = 0.2085, Accuracy = 1.0000\n",
      "Epoch 400: Loss = 0.1517, Accuracy = 1.0000\n",
      "Epoch 410: Loss = 0.0881, Accuracy = 1.0000\n",
      "Epoch 420: Loss = 0.0628, Accuracy = 1.0000\n",
      "Epoch 430: Loss = 0.0368, Accuracy = 1.0000\n",
      "Epoch 440: Loss = 0.0255, Accuracy = 1.0000\n",
      "Epoch 450: Loss = 0.0148, Accuracy = 1.0000\n",
      "Epoch 460: Loss = 0.0083, Accuracy = 1.0000\n",
      "Epoch 470: Loss = 0.0053, Accuracy = 1.0000\n",
      "Epoch 480: Loss = 0.0038, Accuracy = 1.0000\n",
      "Epoch 490: Loss = 0.0018, Accuracy = 1.0000\n",
      "Epoch 500: Loss = 0.0008, Accuracy = 1.0000\n",
      "Epoch 510: Loss = 0.0007, Accuracy = 1.0000\n",
      "Epoch 520: Loss = 0.0005, Accuracy = 1.0000\n",
      "Epoch 530: Loss = 0.0003, Accuracy = 1.0000\n",
      "Epoch 540: Loss = 0.0002, Accuracy = 1.0000\n",
      "Epoch 550: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 560: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 570: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 580: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 590: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 600: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 610: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 620: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 630: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 640: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 650: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 660: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 670: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 680: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 690: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 700: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 710: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 720: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 730: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 740: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 750: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 760: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 770: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 780: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 790: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 800: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 810: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 820: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 830: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 840: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 850: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 860: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 870: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 880: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 890: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 900: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 910: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 920: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 930: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 940: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 950: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 960: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 970: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 980: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 990: Loss = 0.0000, Accuracy = 1.0000\n",
      "\n",
      "Training with Tanh activation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eef8d84230f43a7b4edfe8efd203ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 8.9878, Accuracy = 0.1400\n",
      "Epoch 10: Loss = 7.1421, Accuracy = 0.1500\n",
      "Epoch 20: Loss = 7.0656, Accuracy = 0.3000\n",
      "Epoch 30: Loss = 6.8524, Accuracy = 0.4100\n",
      "Epoch 40: Loss = 6.6143, Accuracy = 0.5300\n",
      "Epoch 50: Loss = 6.0951, Accuracy = 0.6400\n",
      "Epoch 60: Loss = 5.8027, Accuracy = 0.6000\n",
      "Epoch 70: Loss = 5.7977, Accuracy = 0.4900\n",
      "Epoch 80: Loss = 2.5684, Accuracy = 0.2800\n",
      "Epoch 90: Loss = 2.5504, Accuracy = 0.4000\n",
      "Epoch 100: Loss = 2.1852, Accuracy = 0.2900\n",
      "Epoch 110: Loss = 0.4759, Accuracy = 0.6900\n",
      "Epoch 120: Loss = 0.0011, Accuracy = 1.0000\n",
      "Epoch 130: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 140: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 150: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 160: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 170: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 180: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 190: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 200: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 210: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 220: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 230: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 240: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 250: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 260: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 270: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 280: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 290: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 300: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 310: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 320: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 330: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 340: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 350: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 360: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 370: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 380: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 390: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 400: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 410: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 420: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 430: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 440: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 450: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 460: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 470: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 480: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 490: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 500: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 510: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 520: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 530: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 540: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 550: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 560: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 570: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 580: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 590: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 600: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 610: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 620: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 630: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 640: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 650: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 660: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 670: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 680: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 690: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 700: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 710: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 720: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 730: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 740: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 750: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 760: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 770: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 780: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 790: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 800: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 810: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 820: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 830: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 840: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 850: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 860: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 870: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 880: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 890: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 900: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 910: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 920: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 930: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 940: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 950: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 960: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 970: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 980: Loss = 0.0000, Accuracy = 1.0000\n",
      "Epoch 990: Loss = 0.0000, Accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # For demonstration, creating dummy data\n",
    "    np.random.seed(42)\n",
    "    num_samples = 100\n",
    "    input_dim = 3072  # 32 x 32 x 3 flattened image\n",
    "    X_dummy = np.random.rand(num_samples, input_dim)\n",
    "    y_dummy = np.random.randint(0, 10, size=num_samples)\n",
    "    \n",
    "    # Training the network with ReLU activation\n",
    "    print(\"Training with ReLU activation:\")\n",
    "    W1_relu, b1_relu, W2_relu, b2_relu = train_network(\n",
    "        X_dummy, y_dummy, hidden_dim=32, print_every=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining with Sigmoid activation:\")\n",
    "    W1_sig, b1_sig, W2_sig, b2_sig = train_network(\n",
    "        X_dummy, y_dummy, activation='sigmoid', hidden_dim=32, print_every=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining with Tanh activation:\")\n",
    "    W1_tanh, b1_tanh, W2_tanh, b2_tanh = train_network(\n",
    "        X_dummy, y_dummy, activation='tanh', hidden_dim=32, print_every=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X, y, activation='relu', epochs=1000, learning_rate=1e-2,\n",
    "                  hidden_dim=64, print_every=10):\n",
    "    \"\"\"\n",
    "    Train the neural network using the given dataset.\n",
    "    Parameters:\n",
    "      X: Training data of shape (N, 3072)\n",
    "      y: Labels (N,)\n",
    "      activation: Activation function ('relu', 'sigmoid', 'tanh')\n",
    "      epochs: Number of training iterations\n",
    "      learning_rate: Learning rate for optimization\n",
    "      hidden_dim: Number of hidden layer neurons\n",
    "      print_every: Print loss and accuracy every 'print_every' epochs\n",
    "    Returns trained parameters: W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    activation_funcs = {\n",
    "        'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "        'tanh': (tanh, tanh_derivative),\n",
    "        'relu': (relu, relu_derivative)\n",
    "    }\n",
    "\n",
    "    if activation not in activation_funcs:\n",
    "        raise ValueError(\"Unsupported activation function. Choose from 'relu', 'sigmoid', or 'tanh'.\")\n",
    "    \n",
    "    act_func, act_deriv = activation_funcs[activation]\n",
    "    \n",
    "    input_dim = X.shape[1]  # 3072 (32x32x3)\n",
    "    output_dim = 10  # CIFAR-10 has 10 classes\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "        # Forward pass\n",
    "        Z1, A1, scores = forward(X, W1, b1, W2, b2, act_func)\n",
    "        loss, margins = svm_loss(scores, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        dW1, db1, dW2, db2 = backward(X, y, Z1, A1, scores, W2, act_deriv, margins)\n",
    "        \n",
    "        # Updating parameters\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            y_pred = predict(X, W1, b1, W2, b2, act_func)\n",
    "            acc = compute_accuracy(y_pred, y)\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {acc:.4f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(X_test, W1, b1, W2, b2, activation='relu'):\n",
    "    \"\"\"\n",
    "    Test the trained network on test data.\n",
    "    Parameters:\n",
    "      X_test: Test dataset (M, 3072)\n",
    "      W1, b1, W2, b2: Trained model parameters\n",
    "      activation: Activation function used during training\n",
    "    Returns:\n",
    "      Predicted labels for the test set\n",
    "    \"\"\"\n",
    "    activation_funcs = {\n",
    "        'sigmoid': sigmoid,\n",
    "        'tanh': tanh,\n",
    "        'relu': relu\n",
    "    }\n",
    "\n",
    "    if activation not in activation_funcs:\n",
    "        raise ValueError(\"Unsupported activation function. Choose from 'relu', 'sigmoid', or 'tanh'.\")\n",
    "    \n",
    "    act_func = activation_funcs[activation]\n",
    "    \n",
    "    # Forward pass\n",
    "    _, _, scores = forward(X_test, W1, b1, W2, b2, act_func)\n",
    "    predictions = np.argmax(scores, axis=1)  # Getting class with highest score\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Loading CIFAR-10 data\n",
    "    dataset_path = \"./data/cifar-10\"  \n",
    "    X_train, y_train, X_test = load_cifar10_data(dataset_path)\n",
    "\n",
    "    # Training the network using ReLU activation\n",
    "    print(\"Training with ReLU activation:\")\n",
    "    W1_relu, b1_relu, W2_relu, b2_relu = train_network(\n",
    "        X_train, y_train, hidden_dim=64, print_every=10\n",
    "    )\n",
    "\n",
    "    # Evaluating the model on test data\n",
    "    print(\"\\nEvaluating on Test Data:\")\n",
    "    test_predictions = test_network(X_test, W1_relu, b1_relu, W2_relu, b2_relu)\n",
    "    print(f\"Predicted Labels for Test Data: {test_predictions[:10]}\")  # Print first 10 predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
